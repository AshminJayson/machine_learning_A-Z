{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal is to tackle the multi-armed bandit problem by obtaining the right mix of exploring and exploiting\n",
    "- This is the essense of reinforcement learning where we reward the right exploitation and promote exploration\n",
    "\n",
    "- There it is also important to find the best one at the optimal amount of time without wasting time simply exploring\n",
    "\n",
    "- The upper confidence bound algorithm works on the assumption that upon running the exploratory phase for multiple times the final value of the averages will converge to the predicted highest value that may be generated by the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Ads_CTR_Optimisation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "N = 10000\n",
    "d = 10\n",
    "ads_selected = []\n",
    "numbers_of_selections = [0] * d\n",
    "sums_of_rewards = [0] * d\n",
    "total_reward = 0\n",
    "\n",
    "for n in range(N):\n",
    "    ad = 0\n",
    "    max_upper_confidence_bound = 0\n",
    "    for i in range(d):\n",
    "        if numbers_of_selections[i] > 0:\n",
    "            average_reward = (sums_of_rewards[i] / numbers_of_selections[i])\n",
    "            delta_i = math.sqrt(3/2 * math.log(n + 1) / numbers_of_selections[i])\n",
    "            upper_bound = average_reward + delta_i\n",
    "        else:\n",
    "            upper_bound = 1e400\n",
    "        \n",
    "        if upper_bound > max_upper_confidence_bound:\n",
    "            max_upper_confidence_bound = upper_bound\n",
    "            ad = i\n",
    "    ads_selected.append(ad)\n",
    "    numbers_of_selections[ad] += 1\n",
    "    reward_at_n_ad = df.values[n, ad]\n",
    "    sums_of_rewards[ad] += reward_at_n_ad\n",
    "    total_reward += reward_at_n_ad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ads_selected)\n",
    "plt.title('Histogram of ads selected')\n",
    "plt.xlabel('Ads')\n",
    "plt.ylabel('Number of times each ad was selected')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
